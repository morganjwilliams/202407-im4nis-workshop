{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [**Overview**](./00_overview.ipynb) | [**EDA**](./01_EDA.ipynb) | [**Using `sklearn` Models**](./02_LoadModels.ipynb) | [**Making Predictions**](./03_Predictions.ipynb)|\n",
    "| -- | -- | -- | -- |\n",
    "\n",
    "\n",
    "\n",
    "# Exploratory Geochemical Data Analyis in Python\n",
    "\n",
    "This notebook is intended as a lightning introduction to what you can do in Python. In this notebook we'll:\n",
    "* Introduce Python and Jupyter\n",
    "* Introduce some of the key packages we'll be using: `matplotlib`, `Pandas` and `pyrolite`\n",
    "* Load up some data, do some basic analysis, and make some simple plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What is Jupyter?\n",
    "\n",
    "[Jupyter](https://jupyter.org/) is an ecosystem of open source tools which provide interfaces for working with a variety of programming languages. The most well known of these is the Jupyter notebook - which in its simplest form is an electronic notebook consisting of a series of cells (like this one) which can contain a mix of text, code, output, metadata and potentially even interactive elements. Today we're working in Jupyter Lab - which is an environment which combines an interface to notebooks with a file explorer (left) and enables the integration of a variety of other tools.\n",
    "##### Should you use notebooks?\n",
    "\n",
    "Jupyter notebooks can be a good way to organise prototype workflows, and are often a good mechanism for sharing and explaining your code in a way which invites conversation and interaction (hence using them here!). Notably though, they're not necessarily the solution for everything. While you can construct workflows and models through Jupyter notebooks, they are more difficult to manage relative to standalone scripts and libraries when it comes to version management, integration and automation. For this reason it's suggested that once you have something working well, consider writing it up as a separate script or even a Python library/module!\n",
    "\n",
    "##### Using Notebooks for Today (if you haven't seen them before)\n",
    "\n",
    "The key thing to note for today is that it's common to find a mix of text cells like this one (typically written in [Markdown](https://www.markdownguide.org/) for easy markup of text) and code cells (scroll down a bit, they'll have a grey background). While it's not necessary for today, knowing a bit of markdown syntax can help structure notes and documentation accompanying your code. \n",
    "\n",
    "Code cells are not static - here on Binder you can run them (`Shift-Enter` or use the <i class=\"fas fa-play\"></i> button), edit and re-run them! We encourage you to edit, change and break things within reason to get to know the tools (you can always restart Binder!).\n",
    "\n",
    "You can tell which cells are being executed by the notation on the left of it - cells already run will have a number (e.g. `[1]`) noting the order in which it was run, cells yet to run will have an asterisk (`[*]`) and cells which haven't been executed will have empty brackets (`[ ]`). Also check the small circle in the upper right - if it's <i class=\"far fa-circle\"></i> then it's stopped/hasn't started executing, if it's <i class=\"fas fa-circle\"></i> it's trying to execute something/busy. If you get stuck and it looks like nothing's happening the kernel might have stalled; you can restart it under the `Kernel` menu to the top left, using `Restart Kernel...`.\n",
    "\n",
    "<div class='alert alert-warning'> <font color=\"black\"><b>Note:</b> Binder will not save your progress or changes! If you want to keep a modified notebook, you can right click and download from the file browser on the left (or, in Binder - you can also click the download link provided above).</font></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Run me with Shift-Enter/Cmd-Enter!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What is Python?\n",
    "\n",
    "[Python](https://www.python.org/) is a high-level multi-purpose programming language. It's freely and openly available and you'll be able to find a distribution which can run on just about any system (e.g. 'micropython' runs on bare-metal for tiny microcontrollers). There is a large community which uses Python, the majority of which revolves around open-source projects. You can use Python as a fancy calculator, build websites, run servers, build machine learning models, image black holes or provide testing and code generation for an [embedded software framework for NASA](https://github.com/nasa/fprime).\n",
    "\n",
    "Python is an *interpreted* language, which means that rather than being compiled (like e.g. C, C++ and Fortran) it's read, interpreted and executed as needed. For this reason, it'll typically be a bit slower for most task (but not necessarily by much), but it also makes it much less complex to get into, read and run. When working with numerically intense workflows, you're often actually running code which was written in a more performant language in the background - and this bridges a large part of the gap between language 'performance'. Notably, however, Python tends to be written to be later read (or at least it can and should be) - and the accessibility together with it's flexibility are some of the key reasons it's so widely used.\n",
    "\n",
    "You can run Python from the terminal, but typically we want to either write and execute programs (e.g. like 'scripts'; Python is often termed a 'scripting language') or play to the language's strengths and execute code interactively (e.g. in these notebooks!). To do this we need some kind of editor - whether it be notepad, Jupyter Notebooks or a dedicated development environment. While Python is often distributed with some kind of editor, many people have their own favourites - and it tends to depend a bit on what you're doing (e.g. I use 'VS Code', but write these workshops/demonstrations in Jupyter notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geochemical Data Science in Python - `Pandas` and `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to build everything yourself - there is a vast digital library of Python packages you can use for ready-made solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Geochemical Data - `pyrolite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrolite.geochem\n",
    "\n",
    "df = pd.read_csv('../data/basalts/Ueki2018.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geochemical Data Visualisation - Spider Plots, Ternary Plots, +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrolite.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Pb208Pb204\", \"Nd143Nd144\"]].pyroplot.scatter(c=df[\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "for g, gdf in df.groupby(\"Class\"):\n",
    "    ax = gdf[[\"Fe2O3\", \"MgO\", \"TiO2\"]].pyroplot.scatter(ax=ax, alpha=0.2, label=g)\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.pyrochem.normalize_to(\"PM_PON\", units=\"ppm\").pyroplot.spider(\n",
    "    unity_line=True, alpha=0.05, c=df[\"Class\"]\n",
    ")\n",
    "ax.set(ylabel=\"X / Primitive Mantle (Palme and O'Neill)\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.classification import TAS\n",
    "\n",
    "df[\"Na2O + K2O\"] = df[\"Na2O\"] + df[\"K2O\"]\n",
    "cm = TAS()\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "cm.add_to_axes(ax, alpha=0.5, linewidth=0.5, zorder=-1, add_labels=True)\n",
    "df[[\"SiO2\", \"Na2O + K2O\"]].pyroplot.scatter(ax=ax, c=\"k\", alpha=0.2, axlabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TAS\"] = cm.predict(df)\n",
    "df[\"Rocknames\"] = df.TAS.apply(lambda x: cm.fields.get(x, {\"name\": None})[\"name\"])\n",
    "df[\"Rocknames\"].sample(10)  # randomly check 10 sample rocknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "cm.add_to_axes(\n",
    "    ax,\n",
    "    alpha=0.5,\n",
    "    linewidth=0.0,\n",
    "    zorder=-2,\n",
    "    add_labels=False,\n",
    "    which_ids=np.unique(df[\"TAS\"]),\n",
    "    fill=True,\n",
    "    facecolor=[0.9, 0.8, 1.0],\n",
    ")\n",
    "cm.add_to_axes(ax, alpha=0.5, linewidth=0.5, zorder=-1, add_labels=True)\n",
    "df[[\"SiO2\", \"Na2O + K2O\"]].pyroplot.scatter(\n",
    "    ax=ax, c=df[\"TAS\"], alpha=0.7, axlabels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinel_df = pd.read_csv(\"../data/spinel/Schoneveld2020.csv\")\n",
    "spinel_df = spinel_df.rename(\n",
    "    columns={\n",
    "        c: c.replace(\"_apfu\", \"\").replace(\"Fe3\", \"Fe3+\")\n",
    "        for c in spinel_df.columns\n",
    "        if \"_apfu\" in c\n",
    "    }\n",
    ")\n",
    "spinel_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.classification import SpinelTrivalentTernary\n",
    "\n",
    "spinel_cm = SpinelTrivalentTernary()\n",
    "spinel_df[\"ClassifiedPhase\"] = spinel_cm.predict(spinel_df)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax = spinel_cm.add_to_axes(\n",
    "    ax,\n",
    "    alpha=0.5,\n",
    "    linewidth=0.0,\n",
    "    zorder=-2,\n",
    "    add_labels=False,\n",
    "    which_ids=np.unique(spinel_df[\"ClassifiedPhase\"]),\n",
    "    fill=True,\n",
    "    facecolor=[0.9, 0.8, 1.0],\n",
    ")\n",
    "ax = spinel_cm.add_to_axes(ax, alpha=0.5, linewidth=0.5, zorder=-1, add_labels=True)\n",
    "ax = spinel_df[spinel_cm.axis_components].pyroplot.scatter(\n",
    "    ax=ax, c=spinel_df[\"ClassifiedPhase\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Into Machine Learning for Mineral Chemistry\n",
    "\n",
    "Some of the key things to watch out for:\n",
    "* Handling below detection limit data, especially for trace elements \n",
    "* The majority of variation in mineral chemistry is controlled by crystal-chemical factors (in response to geological processes, but in a constrained way); e.g. the first principal components will pick this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Concepts in `scikit-learn`\n",
    "\n",
    "One of the most commonly used frameworks for machine learning in Python is `scikit-learn`, which predominantly focuses on training machine learning models or pipelines from tabular data.  At it's core, you can think of `scikit-learn` as functions which estimate a quantity or label (in code, typically named `y`) based on another set of predictor variables (in code, typically named `X`), such that the model approximates the function $f$ in $y=f(X)$.\n",
    "\n",
    "In `scikit-learn`, the mechanism for training and using model is typically along the lines of (and most commonly exactly as follows):\n",
    "\n",
    "```python\n",
    "model = ModelClassName(<initial_configuation_parameters>) # instantiate a model\n",
    "model.fit(X, y)                                           # train the model to approximate the relationship between X and y\n",
    "```\n",
    "\n",
    "To subsequently use this model to make predictions, you'll typically use code along the lines of:\n",
    "\n",
    "```python\n",
    "predictions = model.predict(X_new) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/rutile/Plavsa2018.csv\").set_index(\"Grain ID\", drop=True)\n",
    "df.pyrochem.elements = df.pyrochem.elements.apply(\n",
    "    pd.to_numeric, errors=\"coerce\"\n",
    ").astype(float)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Phase ID\"].value_counts() / df.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.pyrochem.elements\n",
    "y = df[\"Phase ID\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have some null-data in X, and most `scikit-learn` models don't handle this scenario, we'll need to filter them out (at least for now, and maybe come back to improve how we handle this later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr = ~(pd.isnull(X).sum(axis=1) > 0)\n",
    "X, y = X.loc[fltr, :], y.loc[fltr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we still retain about 95% of our data, which is good to know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr.sum() / fltr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# setting random state so everyone gets the same results\n",
    "random_state = 17\n",
    "classifier = RandomForestClassifier(random_state=random_state)\n",
    "classifier.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some predictions on our dataset and see how well we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X)\n",
    "predictions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[fltr, [\"Phase ID\"]].assign(Predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier does very well, maybe too well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks pretty good, but this wasn't really a fair test - we're using the same data to train the model as we are to examine it; this is referred to as 'data leakage' in ML modeling. Instead what we want to do is keep back a hold-out-set for testing the model which we don't use for training the model.\n",
    "\n",
    "`scikit-learn` has some built in tools for this, thankfully. These allow us to specify the proportion of data we keep for testing, and whether we 'stratify' the dataset such that we have roughly equal proportions of labels in our training and testing sets (in this case, that's probably a good idea). We can also chain `scikit-learn` components together to make a 'pipeline', adding additional preprocessing steps or bringing together/splitting parts of our dataset.\n",
    "\n",
    "We don't do quite as well here, but this is closer to what it might look like in the real world (assuming the things we find are similar to those in our training set, of course..):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyrolite.util.skl.transform import LogTransform\n",
    "\n",
    "\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    stratify=y,\n",
    "    test_size=0.3,\n",
    ")\n",
    "\n",
    "transform = LogTransform()  # log-transform the data\n",
    "clf = RandomForestClassifier(random_state=random_state)  # our classifer model\n",
    "pipe = make_pipeline(transform, clf)\n",
    "pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(XX_train, yy_train).score(XX_test, yy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at how well a classifier model performs is on a per-class basis, such as that used in confusion matrix. In this instance is shows us some new information we wouldn't necessarily have seen otherwise - that our predictions for brookite are the worst, and it's mostly because it's getting misclassified as anatase (~4% of the time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.skl.vis import plot_confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 4))\n",
    "\n",
    "plot_confusion_matrix(pipe, XX_test, yy_test, ax=ax, normalize=True)\n",
    "\n",
    "ax.set_title(\"Polymorph Classifier\\nConfusion Matrix\")\n",
    "ax.set(xlabel=\"Predicted Polymorph\", ylabel=\"True Polymorph\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we're working with random forests, a handy thing to do at this stage might be to look at the relative feature importances (there are reasons why these might not be the most accurate picture, but it gives an idea of how the model is working):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'> <b> Optional Exercise:</b><br> We've blitzed through an example using the mineral geochemistry to identify the titanium phases they represent, but we can also use the information in the dataset  to construct a model around whether those phases are related to mineralization.<br><br>Try altering the code above to make a model based on the 'Mineralized' column of the datset!</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "----\n",
    "\n",
    "| [**Overview**](./00_overview.ipynb) | [**EDA**](./01_EDA.ipynb) | [**Using `sklearn` Models**](./02_LoadModels.ipynb) | [**Making Predictions**](./03_Predictions.ipynb)|\n",
    "| -- | -- | -- | -- |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im4nis-workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
