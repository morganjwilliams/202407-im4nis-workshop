{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [**Overview**](./00_overview.ipynb) | [**From Data Exploration to Machine Learning**](./01_EDA.ipynb) | [**Using `sklearn` Models**](./02_LoadModels.ipynb) | [**Making Predictions**](./03_Predictions.ipynb)|\n",
    "| -- | -- | -- | -- |\n",
    "\n",
    "\n",
    "# Exploratory Geochemical Data Analyis in Python\n",
    "\n",
    "This notebook is intended as a lightning introduction to what you can do in Python. In this notebook we'll:\n",
    "* Introduce Python and Jupyter\n",
    "* Introduce some of the key packages we'll be using: `matplotlib`, `Pandas` and `pyrolite`\n",
    "* Load up some data, do some basic analysis, and make some simple plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What is Jupyter?\n",
    "\n",
    "[Jupyter](https://jupyter.org/) is an ecosystem of open source tools which provide interfaces for working with a variety of programming languages. The most well known of these is the Jupyter notebook - which in its simplest form is an electronic notebook consisting of a series of cells (like this one) which can contain a mix of text, code, output, metadata and potentially even interactive elements. Today we're working in Jupyter Lab - which is an environment which combines an interface to notebooks with a file explorer (left) and enables the integration of a variety of other tools.\n",
    "#### Should you use notebooks?\n",
    "\n",
    "Jupyter notebooks can be a good way to organise prototype workflows, and are often a good mechanism for sharing and explaining your code in a way which invites conversation and interaction (hence using them here!). Notably though, they're not necessarily the solution for everything. While you can construct workflows and models through Jupyter notebooks, they are more difficult to manage relative to standalone scripts and libraries when it comes to version management, integration and automation. For this reason it's suggested that once you have something working well, consider writing it up as a separate script or even a Python library/module!\n",
    "\n",
    "#### Using Notebooks for Today (*if you haven't seen them before*)\n",
    "\n",
    "The key thing to note for today is that it's common to find a mix of text cells like this one (typically written in [Markdown](https://www.markdownguide.org/) for easy markup of text) and code cells (scroll down a bit, they'll have a grey background). While it's not necessary for today, knowing a bit of markdown syntax can help structure notes and documentation accompanying your code. \n",
    "\n",
    "Code cells are not static - here on Binder you can run them (`Shift-Enter` or use the <i class=\"fas fa-play\"></i> button), edit and re-run them! We encourage you to edit, change and break things within reason to get to know the tools (you can always restart Binder!).\n",
    "\n",
    "You can tell which cells are being executed by the notation on the left of it - cells already run will have a number (e.g. `[1]`) noting the order in which it was run, cells yet to run will have an asterisk (`[*]`) and cells which haven't been executed will have empty brackets (`[ ]`). Also check the small circle in the upper right - if it's <i class=\"far fa-circle\"></i> then it's stopped/hasn't started executing, if it's <i class=\"fas fa-circle\"></i> it's trying to execute something/busy. If you get stuck and it looks like nothing's happening the kernel might have stalled; you can restart it under the `Kernel` menu to the top left, using `Restart Kernel...`.\n",
    "\n",
    "<div class='alert alert-warning'> <font color=\"black\"><b>Note:</b> Binder will not save your progress or changes! If you want to keep a modified notebook, you can right click and download from the file browser on the left (or, in Binder - you can also click the download link provided above).</font></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Run me with Shift-Enter/Cmd-Enter!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What is Python?\n",
    "\n",
    "[Python](https://www.python.org/) is a high-level multi-purpose programming language. It's freely and openly available and you'll be able to find a distribution which can run on just about any system (e.g. 'micropython' runs on bare-metal for tiny microcontrollers). There is a large community which uses Python, the majority of which revolves around open-source projects. You can use Python as a fancy calculator, build websites, run servers, build machine learning models, image black holes or provide testing and code generation for an [embedded software framework for NASA](https://github.com/nasa/fprime).\n",
    "\n",
    "Python is an *interpreted* language, which means that rather than being compiled (like e.g. C, C++ and Fortran) it's read, interpreted and executed as needed. For this reason, it'll typically be a bit slower for most task (but not necessarily by much), but it also makes it much less complex to get into, read and run. When working with numerically intense workflows, you're often actually running code which was written in a more performant language in the background - and this bridges a large part of the gap between language 'performance'. Notably, however, Python tends to be written to be later read (or at least it can and should be) - and the accessibility together with it's flexibility are some of the key reasons it's so widely used.\n",
    "\n",
    "You can run Python from the terminal, but typically we want to either write and execute programs (e.g. like 'scripts'; Python is often termed a 'scripting language') or play to the language's strengths and execute code interactively (e.g. in these notebooks!). To do this we need some kind of editor - whether it be notepad, Jupyter Notebooks or a dedicated development environment. While Python is often distributed with some kind of editor, many people have their own favourites - and it tends to depend a bit on what you're doing (e.g. I use 'VS Code', but write these workshops/demonstrations in Jupyter notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geochemical Data Science in Python - `Pandas` and `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great advantages of working with Python is the ecosystem of tools which you can leverage in your own work - from `numpy`, `scipy`, `matplotlib` and `pandas` through to `pyrolite`, which is built upon the others. To be able to use these libraries/packages, you'll need to **import** them. There are a few conventions for importing some common packages to reduce the amount of code you need to type which are handy to recognise. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` is a Python package for working with tabluar data, and in many ways could replace what most folks do in Excel. It provides an interface to your data in such a way that you'll be looking at more than just the numbers (in contrast to `numpy`), and allows you to index, subset, filter and otherwise manipulate your dataset based on indexes - specifically the column names and index values. Like `numpy`, it has some restrictions on the shape of your data, and values within each column all have the same data type. \n",
    "\n",
    "The core objects you'll likely be working with in `pandas` are `pandas.DataFrame`s. You can build dataframes from a variety of sources - including numpy arrays, but also a number of different file types. `pd.to_<format>` functions similarly save our dataframes into other formats. See the [docs](https://pandas.pydata.org/pandas-docs/stable/io.html) for a list of all the file types `pandas` can read and write. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    np.array(\n",
    "        [\n",
    "            [0.6, 1.3, 2.0, 4.1],\n",
    "            [0.2, 1.1, 1.9, 3.2],\n",
    "        ]\n",
    "    ),\n",
    "    columns=[\"A\", \"B\", \"C\", \"D\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a lot of cases, rather than constructing dataframes yourself, you'll likely want to read a file. Later in this notebook we'll pull in some basalt whole-rock chemistry (a dataset targeted at using this chemistry for tectonic discrimination) and spinel geochemistry data from Norilsk, which contains data on the geochemical features of spinels found as inclusions within different phases. Note that `pandas` has a [range of import and export options](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) (and `geopandas` - which we might look at towards the end - has more).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Geochemical Data - `pyrolite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrolite.geochem\n",
    "import pyrolite.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll also import a dataset of basalt composition (from [Ueki, K., Hino, H., Kuwatani, T., 2018. Geochemical Discrimination and Characteristics of Magmatic Tectonic Settings: A Machine-Learning-Based Approach. Geochemistry, Geophysics, Geosystems 19, 1327–1347.](https://doi.org/10.1029/2017GC007401)), with just over 2000 samples taken from global repositories, each with majors, minors, traces and isotopes; this has been modified to record the tectonic setting of each basalt in a 'Class' column. \n",
    "\n",
    "One limitation of `pyrolite` (*at least currently, a workaround is in progress to allow simple units-based naming conventions*) is that most of the geochemical transformation and plotting utilities depend on having elemental or oxide-based naming; luckily, this first this dataset already has column names which fit within the remit of what `pyrolite` will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df = pd.read_csv(\"../data/basalts/Ueki2018.csv\")\n",
    "basalt_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geochemical Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many ways to get to simple bivariate plots, `pyrolite` provides a few options which can provide a simpler interface and easier access to simple styling configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df[[\"MgO\", \"Al2O3\"]].pyroplot.scatter(color=\"k\", marker=\"o\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also readily use variables in our dataset to colour the points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df[[\"MgO\", \"Al2O3\"]].pyroplot.scatter(color=basalt_df[\"Class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we get to larger datasets, overplotting becomes an issue, and we may want to consider methods for visualising the distribution of data as a whole rather than individual points. `pyrolite` has as few options for this, including 'density' plots and 'heatscatter' plots (based on kernel density estimates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df[[\"MgO\", \"Al2O3\"]].pyroplot.density(\n",
    "    bins=100, logx=True, cmap=\"cividis\"\n",
    ")  # we use 'logx=True' here to use a log-spaced grid on x - this stops data density over MgO=0, which is nonsensical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this can look quite nice, and it solves the issue of overplotting we were nearing above, sometimes we want to be able to plot over this and clearly see where new data sits. In this case, we can instead use percentile contours of the kernel density estimate instead (here, at 50th and 95th):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df[[\"MgO\", \"Al2O3\"]].pyroplot.density(\n",
    "    bins=100, contours=[0.5, 0.95], colors=[\"k\", \"0.5\"], logx=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualise your data is to combine summary information provided by the kernel density estimate with the scatter plot to produce a heatscatter plot which retains the best of both worlds. Here we can see the position of each invdividual sample beyond the core of the distribution, and also itentify where the greatest density of samples are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df[[\"MgO\", \"Al2O3\"]].pyroplot.heatscatter(alpha=0.5, logx=True, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ternary Plots\n",
    "\n",
    "Ternary plots are a common in geochemistry, mineralogy and petrology but dont' necessarily pop up elsewhere. `pyrolite` provides an interface to create ternary plots wherever you pass three columns, making it as simple as creating our bivariate plots above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)  # create a subplot to work with\n",
    "\n",
    "for g, gdf in basalt_df.groupby(\"Class\"):  # for each tectonic setting\n",
    "    ax = gdf[[\"Fe2O3\", \"Al2O3\", \"MgO\"]].pyroplot.scatter(\n",
    "        ax=ax, alpha=0.2, label=g\n",
    "    )  # plot the ternary composition\n",
    "\n",
    "ax.legend()  # add a legend to the axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to most ternary plots, however, we can also create data density visualisations (based on distributions in logratio space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df[[\"Fe2O3\", \"Al2O3\", \"MgO\"]].pyroplot.heatscatter(\n",
    "    alpha=0.5, cmap=\"cividis\", s=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spider Plots\n",
    "\n",
    "Visualisation of multivariate patterns in geochemical data can be a challenge, but one tool well adpated to this is the 'spiderplot'. In most cases, you'll want to visualise normalised data (e.g. to Chondrite or Primitive Mantle) such that the effects of nuceleosynthesis and planetary formation are removed and you can instead dig deeper into processes which have happend since. The `pyrolite.pyrochem` API can be chained together with the `pyolite.pyroplot` API to do this in ~ one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = basalt_df.pyrochem.normalize_to(\"PM_PON\", units=\"ppm\").pyroplot.spider(\n",
    "    unity_line=True, alpha=0.05, c=basalt_df[\"Class\"]\n",
    ")\n",
    "\n",
    "ax.set(ylabel=\"X / Primitive Mantle (Palme and O'Neill)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Templates \n",
    "\n",
    "\n",
    "`pyrolite` provides a few built-in plot templates, with the idea to expand the collection in the near future. In most cases, the diagrams are a little more than they seem - being actually built upon a classifer, which will allow you to automatically classify your samples if you have the relevant data. We can easily add them to a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.classification import TAS\n",
    "\n",
    "cm = TAS()\n",
    "cm.add_to_axes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also use them to classify data, if it has the right variables/columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_df[\"Na2O + K2O\"] = basalt_df[\"Na2O\"] + basalt_df[\"K2O\"]\n",
    "basalt_df[\"TAS\"] = cm.predict(basalt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this in our plotting, or elsewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "cm.add_to_axes(\n",
    "    ax,\n",
    "    alpha=0.5,\n",
    "    linewidth=0.0,\n",
    "    zorder=-2,\n",
    "    add_labels=False,\n",
    "    which_ids=np.unique(basalt_df[\"TAS\"]),\n",
    "    fill=True,\n",
    "    facecolor=[0.9, 0.8, 1.0],\n",
    ")\n",
    "cm.add_to_axes(ax, alpha=0.5, linewidth=0.5, zorder=-1, add_labels=True)\n",
    "basalt_df[[\"SiO2\", \"Na2O + K2O\"]].pyroplot.scatter(\n",
    "    ax=ax, c=basalt_df[\"TAS\"], alpha=0.7, axlabels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't limited to bivariate diagrams, we can do the same in a ternary space; we'll first pull in some spinel chemistry data to work with. This data is available as supplementary material in [Schoneveld, L., Barnes, S. J., Williams, M., Le Vaillant, M., and Paterson, D. (2020). Silicate and Oxide Mineral Chemistry and Textures of the Norilsk-Talnakh Ni-Cu-Platinum Group Element Ore-Bearing Intrusions. Economic Geology.](http://doi.org/10.5382/econgeo.4747). We can see that each major-element mineral analysis below includes relevant context as to the data source, analysis, thin section location and the enclosing phase within which the spinel sits. The analyses include major oxides in weight percent, and calcuated atoms per formula unit (apfu) for each of these cations (note we have to rename a few columns in order to work nicely with `pyrolite` here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinel_df = pd.read_csv(\"../data/spinel/Schoneveld2020.csv\")\n",
    "spinel_df = spinel_df.rename(\n",
    "    columns={\n",
    "        c: c.replace(\"_apfu\", \"\").replace(\"Fe3\", \"Fe3+\").replace(\"Fe2\", \"Fe2+\")\n",
    "        for c in spinel_df.columns\n",
    "        if \"_apfu\" in c\n",
    "    }\n",
    ")\n",
    "spinel_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll use the `SpinelTrivalentTernary` template, and check what phases it's classified our spinel as based on their chemistry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.classification import SpinelTrivalentTernary\n",
    "\n",
    "spinel_cm = SpinelTrivalentTernary()\n",
    "spinel_df[\"ClassifiedPhase\"] = spinel_cm.predict(spinel_df)\n",
    "spinel_df[\"ClassifiedPhase\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize this in the ternary space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax = spinel_cm.add_to_axes(\n",
    "    ax,\n",
    "    alpha=0.5,\n",
    "    linewidth=0.0,\n",
    "    zorder=-2,\n",
    "    add_labels=False,\n",
    "    which_ids=np.unique(spinel_df[\"ClassifiedPhase\"]),\n",
    "    fill=True,\n",
    "    facecolor=[0.9, 0.8, 1.0],\n",
    ")\n",
    "ax = spinel_cm.add_to_axes(ax, alpha=0.5, linewidth=0.5, zorder=-1, add_labels=True)\n",
    "ax = spinel_df[spinel_cm.axis_components].pyroplot.scatter(\n",
    "    ax=ax, c=spinel_df[\"ClassifiedPhase\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning for Mineral Chemistry\n",
    "\n",
    "While there are lots of commonalities between using mineral chemistry data to most other tabular datasets (and particularly other geochemical datasets), there are often a few key things to watch out for:\n",
    "* Handling missing data; this includes below detection limit data, especially for trace elements, but also data which simply wasn't measured (common in composite datsets)\n",
    "* In some cases, handling non-normal distributions (dependent on the model used, primarily) - and specifically (to chemistry) compositional dependence\n",
    "* The majority of variation in mineral chemistry is controlled by crystal-chemical factors (in response to geological processes, but in a constrained way); e.g. the first principal components will pick this up\n",
    "\n",
    "For brevity, tor the most part we'll not address these aspects today, and when it comes to missing data, we'll typically just exclude those analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Concepts in `scikit-learn`\n",
    "\n",
    "One of the most commonly used frameworks for machine learning in Python is `scikit-learn`, which predominantly focuses on training machine learning models or pipelines from tabular data.  At it's core, you can think of `scikit-learn` as functions which estimate a quantity or label (in code, typically named `y`) based on another set of predictor variables (in code, typically named `X`), such that the model approximates the function $f$ in $y=f(X)$.\n",
    "\n",
    "In `scikit-learn`, the mechanism for training and using model is typically along the lines of (and most commonly exactly as follows):\n",
    "\n",
    "```python\n",
    "model = ModelClassName(<initial_configuation_parameters>) # instantiate a model\n",
    "model.fit(X, y)                                           # train the model to approximate the relationship between X and y\n",
    "```\n",
    "\n",
    "To subsequently use this model to make predictions, you'll typically use code along the lines of:\n",
    "\n",
    "```python\n",
    "predictions = model.predict(X_new) \n",
    "```\n",
    "\n",
    "Included in this repository is a cleaned version of titanium phase minor + trace element abundances an appendix table from Plavsa et al. (2018), which we'll use here for an initial foray into using machine learning to help with some geological problems. This dataset was collected with addressing potential polymorph-based partitioning effects on mineralization indicators such that titanium phases across a broader range of rock types might be able to be compared. For more, see the paper: [Plavsa, D., Reddy, S. M., Agangi, A., Clark, C., Kylander-Clark, A., & Tiddy, C. J. (2018). Microstructural, trace element and geochronological characterization of TiO2 polymorphs and implications for mineral exploration. Chemical Geology, 476, 130–149.](https://doi.org/10.1016/j.chemgeo.2017.11.011)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/rutile/Plavsa2018.csv\").set_index(\"Grain ID\", drop=True)\n",
    "df.pyrochem.elements = df.pyrochem.elements.apply(\n",
    "    pd.to_numeric, errors=\"coerce\"\n",
    ").astype(float)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Phase ID\"].value_counts() / df.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.pyrochem.elements\n",
    "y = df[\"Phase ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have some null-data in X, and most `scikit-learn` models don't handle this scenario, we'll need to filter them out (at least for now, and maybe come back to improve how we handle this later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr = ~(pd.isnull(X).sum(axis=1) > 0)\n",
    "X, y = X.loc[fltr, :], y.loc[fltr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we still retain about 95% of our data, which is good to know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr.sum() / fltr.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a training dataset, we can train a model. Here we'll use a random forest classifier, which generally adapts and scales well to complexity and data volume. To make sure everyone gets the same results, we'll specify the random seed, which determines any stochastic processes behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_state = 17\n",
    "classifier = RandomForestClassifier(random_state=random_state)\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some predictions on our dataset and see how well we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign these back to the dataframe, so we can view them together if we wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[fltr, [\"Phase ID\"]].assign(Predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how our classifier performs on our dataset using `.score()`, and see that it does very well (maybe too well...)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks pretty good, but this wasn't really a fair test - we're using the same data to train the model as we are to examine it; this is referred to as 'data leakage' in ML modeling. Instead what we want to do is keep back a hold-out-set for testing the model which we don't use for training the model.\n",
    "\n",
    "`scikit-learn` has some built in tools for this, thankfully. These allow us to specify the proportion of data we keep for testing, and whether we 'stratify' the dataset such that we have roughly equal proportions of labels in our training and testing sets (in this case, that's probably a good idea). We can also chain `scikit-learn` components together to make a 'pipeline', adding additional preprocessing steps or bringing together/splitting parts of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyrolite.util.skl.transform import LogTransform\n",
    "\n",
    "\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=17\n",
    ")\n",
    "\n",
    "transform = LogTransform()  # log-transform the data\n",
    "clf = RandomForestClassifier(random_state=random_state)  # our classifer model\n",
    "pipe = make_pipeline(transform, clf)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We don't do quite as well here, but this is closer to what it might look like in the real world (assuming the things we find are similar to those in our training set, of course..):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(XX_train, yy_train).score(XX_test, yy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at how well a classifier model performs is on a per-class basis, such as that used in confusion matrix. In this instance is shows us some new information we wouldn't necessarily have seen otherwise - that our predictions for brookite are the worst, and it's mostly because it's getting misclassified as anatase (~4% of the time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.skl.vis import plot_confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 4))\n",
    "\n",
    "plot_confusion_matrix(pipe, XX_test, yy_test, ax=ax, normalize=True)\n",
    "\n",
    "ax.set_title(\"Polymorph Classifier\\nConfusion Matrix\")\n",
    "ax.set(xlabel=\"Predicted Polymorph\", ylabel=\"True Polymorph\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we're working with random forests, a handy thing to do at this stage might be to look at the relative feature importances - there are reasons why these might not be the most accurate picture, but it gives an idea of how the model is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'> <b> Optional Exercise:</b><br> We've blitzed through an example using the mineral geochemistry to identify the titanium phases they represent, but we can also use the information in the dataset  to construct a model around whether those phases are related to mineralization.<br><br>Try altering the code above to make a model based on the 'Mineralized' column of the datset!</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "----\n",
    "\n",
    "| [**Overview**](./00_overview.ipynb) | [**From Data Exploration to Machine Learning**](./01_EDA.ipynb) | [**Using `sklearn` Models**](./02_LoadModels.ipynb) | [**Making Predictions**](./03_Predictions.ipynb)|\n",
    "| -- | -- | -- | -- |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im4nis-workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
