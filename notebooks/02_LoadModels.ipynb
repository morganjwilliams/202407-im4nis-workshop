{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [**Overview**](./00_overview.ipynb) | [**From Data Exploration to Machine Learning**](./01_EDA.ipynb) | [**Using `sklearn` Models**](./02_LoadModels.ipynb) | [**Making Predictions**](./03_Predictions.ipynb)|\n",
    "| -- | -- | -- | -- |\n",
    "\n",
    "# Loading and Using Existing `sklearn` Models\n",
    "\n",
    "In this notebook we'll:\n",
    "* Upload some serialized models which have already been trained to Jupyter Lab\n",
    "* Load these models into `sklearn` object using `joblib`\n",
    "\n",
    "Note: If you haven't already, run the download notebook [here](../data/DownloadData.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing existing models, the key aspect for `sklearn` is that the versions in an ideal case would match; when you try to load models with an inconsistent version, you'll get a warning (in the best case) and may get an error. The model files for the IM4NiS project were built with `scikit-learn v1.1.3` (now a bit out of date); this was the default version installed in this environment (at least via Binder, or if you used the `environment.yml` file associated with this notebook). We can check the version of `sklearn` we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've verified we have the right version of `sklearn`, we can load up a file in `joblib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "clf = joblib.load( # this is a classifier file, hence i've named it clf here- you could call it whatever you like, as long as you're consistent\n",
    "    \"../data/MachineLearningModels/Spinel_LAICPMS_Binary_Mineralization_Classifier.joblib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classifier is a histogram-gradient-boosted classifier (a fancy form of random forest), and any of the parameters set on it's instantiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check what the features the clasisfier was trained on were, in case we didn't have the training dataset handy (in this case, we do, but it's stil good to check). One thing to note about this list of features is that they include some things which are not likely provided as standard in most datasets, so we might have to calculate them. In the case of the spinel LAICPMS-based clasisifer, this list includes `lambdas`, which parameterize REE profiles; you can [read a bit more about them in the pyrolite documentation](https://pyrolite.readthedocs.io/en/main/examples/geochem/lambdas.html), including how to calculate them and associated anomalies should you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what classes the classifier predicts, noting here we expect a binary mineralized/unmineralized class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier exposes a `.predict()` method, which we'll use to classify any new data (next notebook), or in testing our model (below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data, Introspection and Model Performance\n",
    "\n",
    "As the training and testing data is provided in DAP (which we've also downloaded here), you can independently conduct model introspection and performance evaluation in the same way we've done it during the project. Here we use a convenience function (`get_model_data()`) which fetches related data for you, and puts it in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_model_data\n",
    "\n",
    "clf_data = get_model_data(\"Spinel_LAICPMS_Binary_Mineralization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can pull out the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf_data['Classifier']\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some of the data used to train and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_data['XX_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly use the classifier, to make predictions or otherwise, e.g. on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(clf_data['XX_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we know the appropriate labels for this test dataset, we can also use it to score the model (i.e., determine the performance/accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(clf_data['XX_test'], clf_data['yy_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at model performance in a generalised and visual way is to use confusion matricies; there's a convenience function in `pyrolite` to quickly make one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.skl.vis import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(clf_data['Classifier'],\n",
    "                      clf_data['XX_test'], \n",
    "                      clf_data['yy_test'], \n",
    "                      normalize=True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at things like feature importance; here using permutation importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import plot_permutation_importances\n",
    "\n",
    "plot_permutation_importances(clf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "| [**Overview**](./00_overview.ipynb) | [**From Data Exploration to Machine Learning**](./01_EDA.ipynb) | [**Using `sklearn` Models**](./02_LoadModels.ipynb) | [**Making Predictions**](./03_Predictions.ipynb)|\n",
    "| -- | -- | -- | -- |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im4nis-workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
